---
title: "Byzantine、PBFT"
date: 2021-10-16T22:32:43+08:00
draft: true
toc: true
images:
tags: 
  - distributed
---

**Practical Byzantine Fault Tolerance,PBFT，实用拜占庭容错算法**，在1999年召开的第三届操作系统设计与实现研讨会上有 Miguel Castro 和 Barbara Liskov 提出，(经常两个人)，可以用于异步网络，并且在前人所做工作的基础上大幅度提高了系统的响应效率，具有较强的实用性，强啊。

**PBFT 算法假设的环境比 Raft 算法更恶劣，Raft 算法只支持容忍故障节点，PBTF算法 还支持拜占庭节点(叛徒节点)。**

**PBFT 算法中假设系统中存在一个恶意节点**，可以伪装成自身发生了故障或与其他节点的通信产生延迟，以便对整个系统造成最大的破坏。恶意节点造成的破坏在计算机上是受限的，**不能破坏节点间用到的密码学技术**。

**拜占庭将军问题**是 Leslie Lamport 在10世纪80年代提出的一个假想问题，将军中有叛徒存在时，如何使得将军们**在存在叛徒的非信任环境中建立对战斗计划的共识**。

**共识算法的核心是在正常的节点间形成对网络形态的共识。**

**PBFT 使用了较少的预选将军数，运行非常高效，交易量和吞吐量比较高**，如果有太多的将军，彼此交流信息的时候产生大量的消耗，性能会大幅下降。

## 节点角色

**PBFT 算法是一种基于状态机复制的共识算法，节点角色有主节点和副本，两种角色可以相互转换。**

**Primary 节点和 Replica 的转换引入了视图(view)，在每个视图中，只有一个副本为主节点，每当主节点发生变更时，其所对应的视图会随之变化，视图在PBFT算法中起到了逻辑时钟的作用。**

PBFT 对每个副本提出了两个限定条件

1. **所有节点必须是确定的，在相同状态下给定一组参数，最终的执行结果必须相同**
2. **所有节点的起始状态必须一致**

**PBFT 最大的容错即节点数量是 (n-1)/3，Raft 最大容错节点是 (n-1)/2**

## 算法容错

n 个节点，异常节点 f 个，正常节点有 n-f 个，只要收到 n-f 个就能做出决定。

n-f 个节点中也包括因为故障而不能及时回复的消息(非恶意)，正确的节点就是  n-f-f 个，为了一致性，正确的节点占多数，最坏的情况正确的节点数为 n-f-f>f 个， n 最少为 3f +1。

## 算法流程

PBFT 初始阶段，**主节点由公式 p = v mod n 计算得出，p 是主节点编号，v 是视图编号，n 是集群中节点的个数**。随着 v 的增长 p 不断变化(可优化)，算法采用轮流坐庄的方法。

**客户端发送消息 m 给主节点 p 时，主节点就开始了 PBFT 协议的三阶段，预准备、准备、提交**

预准备、准备阶段最重要的任务是**保证同一个主节点发出的请求在同一个视图中的顺序是一致的**，**prepare 和 commit 阶段最重要的任务是保证请求在不同视图之间的顺序是一样的**。

- **主节点**收到客户端消息后，**构造 pre-prepare 阶段的消息结构体**<PRE-PREPARE,v,n,d>,m> 并**广播**到集群中的其他节点
  - PRE-PREPARE 标示当前消息所处的协议阶段
  - v标识当前消息所在的视图编号
  - n为主节点广播消息的唯一递增编号
  - d为消息m的消息摘要
  - m为客户端发来的消息
- **副本收到主节点消息**后，会对消息进行**有效性检查**，**检查通过会将消息存储在本节点中**，同时副本会进入Prepare 阶段，广播消息<PrePARE,v,n,d,i>>,其中 **i 是本节点的编号**，对消息的有效性检查包括以下几个方面
  - 检查收到的消息体中**消息摘要** d ，是否和自己对消息 m 生成的**消息摘要一致**，确保消息的完整性
  - 检查v是否和节点当前**视图编号**v一致
  - 检查编号n是否在水线 h ～H 之间，避免恶意节点快速消耗可用编号
  - 检查之间是否收到过相同编号 n 和 v，但是不同消息摘要 d 的消息
- **副本收到 2f+1 (包括自己)个一致的PREPARE 消息后，会进入 commit 阶段**，并且**广播消息<COMMIT,v,n,D(m),i>** 给集群中的其他节点。再收到PREPARE消息后，副本**同样会对消息进行有效性检查**，检查的内容是副本收到主节点消息后进行检查的**前三点**。
- **副本收到2f+1(包括自己)个一致的 COMMIT 消息后执行 m 中包含的操作**，如果有多个m则按照编号n从小到大执行，执行完毕后发送执行成功的消息给客户端。

## 小总结

消息通信在后面阶段越来越多，随着客户端消息或者集群节点的增加，集群整体消息感觉是呈线性锯齿状递增或趋于稳定，感觉这个抖动比较厉害，会有消息突刺的现象，**有一种分代垃圾回收年轻代的感觉**。

## 日志压缩

集群都会有日志，同步状态传递消息、集群状态变更的日志同步，日志总是不能无限增长的。

PBFT 算法**采用检查点(Checkpoint)**机制来压缩日志，本质是和Raft算法采用快照的方式清理日志是一样的，实现不同。

**检查点的含义是当前节点所处理的最新请求编号，大部分节点(2f+1) 已经共识完成的最大请求编号被称为稳定检查点(Stable Checkpoint)**。

为每一个操作创建一个集群中的检查点的代价比较高，所以PBFT会**为每常数个操作创建一个检查点**，**当这个检查点得到集群中多数节点的认可以后，就变成了稳定检查点，稳定检查点的日志之前的日志就成为了过时日志，可以删除**，真好啊，比Raft 好。

当节点 i **生成检查点后会广播消息**<**CHECKPOINT**,n,d,i>，其中 n 是 最后一次执行的消息编号，d 是 n 执行后的状态及状态的摘要。每个节点收到 2f + 1 个相同的 n 和 d 的 CHECKPOINT 消息之后，**检查点就变成了稳定检查点，同时删除本地编号小于或等于n的消息**。

稳定检查点还有**提高水位线(Water Mark)**的作用，当一个稳定检查点被创建的时候，水位低位h 被修改为稳定检查点消息的编号n，水位高位 H 被修改为 h + k，k 是之前用于创建检查点的间隔常数。节点接受消息的编号 n 必须在 水线 h ～ H 之间，主要是**为了防止一个失效节点使用一个很大的编号从而消耗消息的编号空间。**<? todo，**消息的编号空间要求这么苛刻吗**>

## 视图切换

PBFT 算法的正常流程是主节点广播客户端的消息请求到集群，如果主节点宕机，**视图切换(View-change)机**制提供了一种当主节点出现异常以后依然可以保证集群高可用的机制，**通过计时器来进行切换避免副本长时间等待**。

副本收到请求时，启动一个计时器，如果这个时候刚好有计时器在运行就重置计时器。当主节点宕机的时候，副本 i 会在当前视图 v 中超时，这个时候副本 i 就会触发视图切换的操作，将视图切换为 v + 1。

- 副本 i 停止接受除检查点消息，视图切换和新视图变更(New View-change)以外的请求，

  同时广播消息<VIEW-CHANGE，v+1，n，C，P，i> 到集群

  - n 是节点 i 知道的最后一个稳定检查点的消息编号
  - C 是节点 i 保存的经过 2f + 1 节点确认的稳定检查点的消息集合
  - P 是保存了 n 之后所有已经达到 Prepare 阶段消息的集合

- 当在视图 v + 1 中的主节点  p1 收到 2f 个有效的将视图变更为 v + 1的消息以后，p1就会广播消息 <NEW-VIEW，v+1，V，Q>  

  - V 是 P1 收到的、包括自己发送的视图切换的消息集合
  - Q 是 Pre-prepare 阶段的消息集合，PRE-PREPARE 消息是从 PREPARE 消息转换过来的

- 从节点接受到NEW-VIEW消息后，验证签名，判断 V 和 Q 中的消息是否合法，验证通过后主节点和副本都进入视图 v + 1。当 p1 收到 2f + 1 个 VIEW-CHANGE 消息以后，可以确定稳定检查点之前的消息在视图切换的过程中不会丢失。

  但是稳定检查点之后，下一个检查点之前的PREPARE消息可能会被丢弃。

  在视图切换到 v +1 后，PBFT 算法会把旧视图中的 PREPARE 消息变为 PRE-PREPARE 消息然后广播。

- 如果集合 p 为空，广播消息 <PRE-PREPARE，v+1，n，null>

- 如果集合P不为空，广播消息<PRE-PREPARE，v+1，n，d>

**视图切换机制最为重要的就是 C、P、Q 三个消息的集合，Pre-prepare 和 prepare 阶段最重要的任务就是保证同一个节点发出的请求在同一个视图中的顺序是一致的。**

- C 确保了视图变更的时候，稳定检查点之前的状态安全

- P 确保了视图变更前，PREPARE 消息的安全
- Q确保了视图变更后，P集合中的消息安全

没看懂，但是找到了一个大佬的文章，写的很详细，周末在看<TODO>

[周末看大彬老师的PBFT-VIEW-CHANGE](https://lessisbetter.site/2020/03/22/why-pbft-needs-viewchange/)

## 主动恢复

传统的PBFT 算法没有实现主动恢复的功能（1999年[初次发布](http://pmg.csail.mit.edu/papers/osdi99.pdf)），主动恢复的节点会索取网络中其他节点的视图和最新的区块高度等信息来更新自身的状态，以此来保证与网络中其他节点一致([2001年，PBFT-PR](http://www.pmg.csail.mit.edu/papers/bft-tocs.pdf))，**Proactive Recovery**。

**Raft 算法中采用 leader 记录每个 follower 提交的日志编号，再发送心跳包时携带额外的信息方式来保持同步，PBFT 采用了 视图协商的机制来保持同步。**

当**节点日志落后太多**，可能因为网络、磁盘等原因，当它收到主节点发来的消息后，**对消息水线的检查就会失败**，导致**计时器超时**，从而**发送视图变更的请求**，但是只有自己一个节点发送视图变更的请求，**请求消息的数量达不到 2f + 1，使得本来正常运行的节点退化为恶意节点**，尽管时非主观原因导致的，但是为了尽可能保证集群的稳定性，PBFT 加入了视图协商机制。

**当一个节点多次尝试视图变更失败后，就会触发视图协商机制来同步集群数据。**

- 新增节点 Replica Max 发起 NegotiateView 消息给其他节点
- 集群中其他节点收到消息以后，返回自己的视图信息、节点ID和节点总数N
- 新增节点 Replica Max 收到 2f + 1 个相同的消息后，如果 2f + 1 个视图编号和自己不同，则同步视图信息和N
- Replica Max 同步完视图信息后，发送 RecoveryToC heckPoint 消息，其中包含自身的检查点信息
- 其他节点收到 RecoveryToCheckpoint 消息后将自身最新的检查点信息返回给 Replica Max 
- Replica Max 2f + 1 个消息后，更新自己的检查点到最新，更新完成以后向正常节点索要 P、Q 和 C 的信息(PBFT 算法中 Pre-prepare 阶段、Prepare 阶段和 Commit 阶段的数据)同步至全网最新状态。

## 节点配置更新
